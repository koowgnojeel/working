Feature extraction transforms
raw data (images, text) into meaningful numerical features
to simplify modeling.

Embeddings are a specific,
learned type of feature representation,
usually dense, lower-dimensional vectors produced by neural networks
that capture complex semantic relationships (e.g., Word2Vec, BERT).

All embeddings are features,
but not all features are embeddings. 

Key Differences Between Feature Extraction and Embeddings

    * Definition
      Feature extraction extracts raw,
      often human-interpretable characteristics (e.g., pixel intensity, word counts).

      Embeddings are high-dimensional mapping into a dense, lower-dimensional space
      learned by a model.

    * Methodology
      Feature extraction can be manual or rule-based (e.g., TF-IDF, HOG).
      Embeddings are learned through training on specific tasks (e.g., Word2Vec, CNNs).

    * Structure
      Extracted features are often sparse and high-dimensional.
      Embeddings are dense, compact, and fixed-length,
      making them more efficient for downstream tasks.

    * Context
      Feature extraction is generally used for simpler data reduction,
      while embeddings are used to represent complex, abstract,
      or semantic relationships in data (e.g., finding synonyms).

This video explains the relationship between feature extraction and word embeddings:

    Embedding Layer | Word Embedding Technique | Feature Extraction vs Word Embedding
    https://www.youtube.com/watch?v=wwz93b-OMf0&t=10s

* Feature Extraction (Traditional Approach)

    * Purpose
      Simplify raw data, reduce dimensionality, and highlight important information.

    * Examples
      Bag of Words, TF-IDF, HOG, color histograms.

    * Pros
      Often faster, interpretable, does not require a training dataset.

* Embeddings (Modern Approach)

    * Purpose
      Map data to a semantic space
      where similar items are closer together.

    * Examples
      Word2Vec, GloVe, BERT, image embeddings (ResNet).

    * Pros
      Generalizes well across tasks,
      captures non-linear,
      deep semantic relationships.

Which to Use?

    Use feature extraction (like PCA or TF-IDF)
    when you have limited training data, working with simpler, non-sequential data.

    Use embeddings (like Word2Vec or BERT)
    to capture complex, semantic, or relational information in text, image, or graph data,
    particularly with modern neural network architectures.



Etc.

    About Scikit-learn

        Scikit-learn does not have
        a general-purpose, built-in "embedding layer" for training representations
        in the same way neural network libraries (like TensorFlow or PyTorch) do.

        Instead, it offers various methods
        for feature extraction and dimensionality reduction
        which can produce numerical representations (embeddings)
        from raw data or existing features.

        * Text Feature Extraction
          These tools convert raw text into numerical features,
          a necessary step for machine learning algorithms.

          CountVectorizer, TfidfVectorizer, FeatureHasher

        * Dimensionality Reduction
          These methods can be used to transform
          high-dimensional data (including features generated by the text vectorizers)
          into lower-dimensional,
          dense representations (embeddings)
          while preserving important structures.

            * Manifold Learning (sklearn.manifold):
                * TSNE (t-Distributed Stochastic Neighbor Embedding)
                * Isomap
                * LocallyLinearEmbedding (LLE)
                * MDS (Multidimensional Scaling)
                * SpectralEmbedding

            * Matrix Factorization (sklearn.decomposition):

                * PCA (Principal Component Analysis); A classic method for linear dimensionality reduction.
                * TruncatedSVD (Latent Semantic Analysis)
                * DictionaryLearning and FactorAnalysis.

            * Ensemble Methods (sklearn.ensemble):

                * RandomTreesEmbedding
                  An unsupervised method
                  that transforms data into a sparse, high-dimensional representation
                  using a forest of completely random trees.

        * External Libraries
          For modern,
          pre-trained word embeddings from large language models (LLMs),
          users typically rely on
          external libraries (such as sentence-transformers or specific APIs)
          and then use scikit-learn for subsequent tasks
          like clustering or classification.

          Some third-party packages,
          like scikit-embeddings or pytorch-tabular,
          also provide scikit-learn-compatible wrappers
          for training or utilizing neural network embeddings within scikit-learn pipelines.


